{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ‡¨ğŸ‡¿ Czech Character-Level GPT Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_PATH = \"checkpoint_250chinchilla\" \n",
    "TOKENIZER_PATH = \"checkpoint_250chinchilla/tokenizer.json\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoConfig, AutoModelForCausalLM\n",
    "from tokenizers import Tokenizer\n",
    "from tqdm.auto import tqdm\n",
    "import textwrap\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import CzechGPTConfig, CzechGPTModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "AutoConfig.register(\"czech-gpt\", CzechGPTConfig)\n",
    "AutoModelForCausalLM.register(CzechGPTConfig, CzechGPTModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:1\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda:1\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CzechGPTModel(\n",
       "  (embed): ModuleDict(\n",
       "    (wte): Embedding(64, 384)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (body): ModuleDict(\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x Block(\n",
       "        (ln_1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=384, out_features=1152, bias=True)\n",
       "          (c_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (qk_norm): QKNorm(\n",
       "            (q_norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "            (k_norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "          (rope): RoPE()\n",
       "        )\n",
       "        (ln_2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (c_proj): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (head): Linear(in_features=384, out_features=64, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    CHECKPOINT_PATH, \n",
    "    trust_remote_code=True\n",
    ")\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer.from_file(TOKENIZER_PATH)\n",
    "pad_token_id = tokenizer.get_vocab().get(\"[PAD]\", 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_LENGTH = model.config.context_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_wrapped(text):\n",
    "    print(textwrap.fill(text, width=100))\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
    "    text = re.sub(r'\\[.*?\\]', '', text)\n",
    "    text = re.sub(r'[^a-zÃ¡ÄÄÃ©Ä›Ã­ÅˆÃ³Å™Å¡Å¥ÃºÅ¯Ã½Å¾0-9\\s.,!?-]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def generate_from_prompt(prompt, max_new_chars=80, temperature=0.7, top_k=50):\n",
    "    cleaned_prompt = clean_text(prompt)\n",
    "    \n",
    "    if len(cleaned_prompt) > CONTEXT_LENGTH:\n",
    "        print(f\"Warning: Cleaned prompt is too long ({len(cleaned_prompt)} > {CONTEXT_LENGTH} chars). Truncating.\")\n",
    "        cleaned_prompt = cleaned_prompt[-CONTEXT_LENGTH:]\n",
    "    token_ids = tokenizer.encode(cleaned_prompt).ids\n",
    "    input_ids = torch.tensor([token_ids], dtype=torch.long, device=device)\n",
    "    output_ids = model.generate(\n",
    "        input_ids, max_new_tokens=max_new_chars, temperature=temperature,\n",
    "        top_k=top_k, do_sample=True, pad_token_id=pad_token_id\n",
    "    )\n",
    "    full_output = tokenizer.decode(output_ids[0].tolist(), skip_special_tokens=True)\n",
    "    return full_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: General Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_prompts = [\n",
    "    \"Byl jednou jeden starÃ½ hrad. V noci se z nÄ›j ozÃ½valy podivnÃ© zvuky. Jednoho dne se odvÃ¡Å¾nÃ½ rytÃ­Å™ rozhodl,\",\n",
    "    \"Recept na palaÄinky:\\n1. SmÃ­chejte 250g hladkÃ© mouky a 2 vejce.\\n2. PÅ™idejte 500ml mlÃ©ka a Å¡petku soli.\\n3.\",\n",
    "    \"Na obloze se objevila duha. MÄ›la vÅ¡echny barvy:\",\n",
    "    \"V roce 2050 bude svÄ›t vypadat ÃºplnÄ› jinak. Auta budou lÃ©tat a lidÃ© budou\",\n",
    "    \"PÅ™Ã­Äinou globÃ¡lnÃ­ho oteplovÃ¡nÃ­ je pÅ™edevÅ¡Ã­m\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "GENERAL TEXT GENERATION\n",
      "========================================\n",
      "\n",
      "--- PROMPT 1 ---\n",
      "PROMPT: Byl jednou jeden starÃ½ hrad. V noci se z nÄ›j ozÃ½valy podivnÃ© zvuky. Jednoho dne se odvÃ¡Å¾nÃ½\n",
      "rytÃ­Å™ rozhodl,\n",
      "MODEL: byl jednou jeden starÃ½ hrad. v noci se z nÄ›j ozÃ½valy podivnÃ© zvuky. jednoho dne se odvÃ¡Å¾nÃ½\n",
      "rytÃ­Å™ rozhodl,Å¾ esyi Ä›oyvÄ eo h ola-u y-u ojd yue rn aza,kesvly   e aa. n 000 eeov  ercmc 09 anio eo\n",
      "ec1 9 nb ee eo 9..n10 rk15  Å¡rup\n",
      "\n",
      "--- PROMPT 2 ---\n",
      "PROMPT: Recept na palaÄinky: 1. SmÃ­chejte 250g hladkÃ© mouky a 2 vejce. 2. PÅ™idejte 500ml mlÃ©ka a\n",
      "Å¡petku soli. 3.\n",
      "MODEL: recept na palaÄinky 1. smÃ­chejte 250g hladkÃ© mouky a 2 vejce. 2. pÅ™idejte 500ml mlÃ©ka a\n",
      "Å¡petku soli. 3.pokdjaurv eibr ebt osaa oa eb et aegsrfeeu dnshnvld rdkmvdt nkbre yue rcene yese edt\n",
      "soige ai etale yee crmnodt klb  vrt\n",
      "\n",
      "--- PROMPT 3 ---\n",
      "PROMPT: Na obloze se objevila duha. MÄ›la vÅ¡echny barvy:\n",
      "MODEL: na obloze se objevila duha. mÄ›la vÅ¡echny barvy  s Ã­Ã¡ksmrnmnmnn,ke tetv. oljÅ¡eoÃ¡ Ã¡itozk 01 y i\n",
      "erntt kaeats asrpi s etinsÃ½i kÄ›Ã­ae s osd  e uÄ s Ã½mvu enyv.ojsatj  eiu\n",
      "\n",
      "--- PROMPT 4 ---\n",
      "PROMPT: V roce 2050 bude svÄ›t vypadat ÃºplnÄ› jinak. Auta budou lÃ©tat a lidÃ© budou\n",
      "MODEL: v roce 2050 bude svÄ›t vypadat ÃºplnÄ› jinak. auta budou lÃ©tat a lidÃ© budousoi tkÃ­octot\n",
      "ep.\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"soÄntntn Ã­Ã¡azj elxbr Ã¡ua e.cma ee  lt afiizj ipe e ee aaosn onmnb Å¯ajz e 7a06ad\n",
      "l haua\n",
      "\n",
      "--- PROMPT 5 ---\n",
      "PROMPT: PÅ™Ã­Äinou globÃ¡lnÃ­ho oteplovÃ¡nÃ­ je pÅ™edevÅ¡Ã­m\n",
      "MODEL: pÅ™Ã­Äinou globÃ¡lnÃ­ho oteplovÃ¡nÃ­ je pÅ™edevÅ¡Ã­mpoatÃ­oatlntdsÃ½h oe.\"\"\"\"pesrv ed sa  an otaaessio\n",
      "oe  knhe e nca ps eee p ei yor ecoo edt yrnhle ercoe oe clvnksvn ataia\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*40)\n",
    "print(\"GENERAL TEXT GENERATION\")\n",
    "print(\"=\"*40)\n",
    "for i, prompt in enumerate(general_prompts):\n",
    "    print(f\"\\n--- PROMPT {i+1} ---\")\n",
    "    print_wrapped(f\"PROMPT: {prompt}\")\n",
    "    generated_text = generate_from_prompt(prompt, max_new_chars=120, temperature=0.75)\n",
    "    print_wrapped(f\"MODEL: {generated_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: In-Domain Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_domain_questions = {\n",
    "    \"HlavnÃ­ mÄ›sto ÄŒR\": [\n",
    "        \"OtÃ¡zka: JakÃ© je hlavnÃ­ mÄ›sto ÄŒeskÃ© republiky?\\nOdpovÄ›Ä:\",\n",
    "        \"HlavnÃ­ mÄ›sto ÄŒeskÃ© republiky je\",\n",
    "        \"Centrem politickÃ©ho a kulturnÃ­ho dÄ›nÃ­ v ÄŒesku je mÄ›sto zvanÃ©\"\n",
    "    ],\n",
    "    \"Autor BabiÄky\": [\n",
    "        \"Kdo je autorem knihy BabiÄka?\\nOdpovÄ›Ä:\",\n",
    "        \"DÃ­lo BabiÄka napsala slavnÃ¡ ÄeskÃ¡ spisovatelka\",\n",
    "        \"RomÃ¡n o Å¾ivotÄ› na StarÃ©m bÄ›lidle napsal(a)\"\n",
    "    ],\n",
    "    \"Skladatel MÃ© vlasti\": [\n",
    "        \"KterÃ½ skladatel sloÅ¾il cyklus symfonickÃ½ch bÃ¡snÃ­ MÃ¡ vlast?\\nOdpovÄ›Ä:\",\n",
    "        \"Autorem MÃ© vlasti je\",\n",
    "        \"Hudbu k VltavÄ› sloÅ¾il\"\n",
    "    ],\n",
    "    \"Karel IV.\": [\n",
    "        \"JakÃ© bylo pÅ¯vodnÃ­ jmÃ©no Karla IV.?\\nOdpovÄ›Ä:\",\n",
    "        \"Otec vlasti, Karel IV., se pÅ¯vodnÄ› jmenoval\",\n",
    "        \"KdyÅ¾ se narodil syn EliÅ¡ky PÅ™emyslovny, dostal jmÃ©no\"\n",
    "    ],\n",
    "    \"NejvyÅ¡Å¡Ã­ hora ÄŒR\": [\n",
    "        \"Jak se jmenuje nejvyÅ¡Å¡Ã­ hora v ÄŒeskÃ© republice?\\nOdpovÄ›Ä:\",\n",
    "        \"NejvyÅ¡Å¡Ã­ vrchol ÄŒeska je\",\n",
    "        \"Vrchol, kterÃ½ v KrkonoÅ¡Ã­ch dosahuje vÃ½Å¡ky 1603 m n. m., se nazÃ½vÃ¡\"\n",
    "    ]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "TOPIC: HlavnÃ­ mÄ›sto ÄŒR\n",
      "========================================\n",
      "PROMPT: OtÃ¡zka: JakÃ© je hlavnÃ­ mÄ›sto ÄŒeskÃ© republiky? OdpovÄ›Ä:\n",
      "MODEL: otÃ¡zka jakÃ© je hlavnÃ­ mÄ›sto ÄeskÃ© republiky? odpovÄ›Än aplk eie ee  ei ee e e e e e  e  e  e\n",
      "e   e  e\n",
      "PROMPT: HlavnÃ­ mÄ›sto ÄŒeskÃ© republiky je\n",
      "MODEL: hlavnÃ­ mÄ›sto ÄeskÃ© republiky jepia amr eret o ocrs  ee  ee ee ee ee ee ee ee e e\n",
      "PROMPT: Centrem politickÃ©ho a kulturnÃ­ho dÄ›nÃ­ v ÄŒesku je mÄ›sto zvanÃ©\n",
      "MODEL: centrem politickÃ©ho a kulturnÃ­ho dÄ›nÃ­ v Äesku je mÄ›sto zvanÃ©t oe\n",
      "aataa.\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
      "\n",
      "\n",
      "========================================\n",
      "TOPIC: Autor BabiÄky\n",
      "========================================\n",
      "PROMPT: Kdo je autorem knihy BabiÄka? OdpovÄ›Ä:\n",
      "MODEL: kdo je autorem knihy babiÄka? odpovÄ›Än as at. p ei ee e e e e  e    e e  e e     e e e\n",
      "PROMPT: DÃ­lo BabiÄka napsala slavnÃ¡ ÄeskÃ¡ spisovatelka\n",
      "MODEL: dÃ­lo babiÄka napsala slavnÃ¡ ÄeskÃ¡ spisovatelkaasrcat eie eee ee  o ocrsu dl eiee ee ee e ee e\n",
      "ee\n",
      "PROMPT: RomÃ¡n o Å¾ivotÄ› na StarÃ©m bÄ›lidle napsal(a)\n",
      "MODEL: romÃ¡n o Å¾ivotÄ› na starÃ©m bÄ›lidle napsalas  etea   o eie eee  o otik   oe ee  o oi ee ee  e\n",
      "\n",
      "\n",
      "========================================\n",
      "TOPIC: Skladatel MÃ© vlasti\n",
      "========================================\n",
      "PROMPT: KterÃ½ skladatel sloÅ¾il cyklus symfonickÃ½ch bÃ¡snÃ­ MÃ¡ vlast? OdpovÄ›Ä:\n",
      "MODEL: kterÃ½ skladatel sloÅ¾il cyklus symfonickÃ½ch bÃ¡snÃ­ mÃ¡ vlast? odpovÄ›Än apiaoi  aie   ee  eteni\n",
      "ee  ee   ee e e e e e\n",
      "PROMPT: Autorem MÃ© vlasti je\n",
      "MODEL: autorem mÃ© vlasti jepih eie  ei ee e e e  e ee e e e eee  e e e   e ee\n",
      "PROMPT: Hudbu k VltavÄ› sloÅ¾il\n",
      "MODEL: hudbu k vltavÄ› sloÅ¾il   eie ee   o eiee  ee o olm ade e ee e ee   er e\n",
      "\n",
      "\n",
      "========================================\n",
      "TOPIC: Karel IV.\n",
      "========================================\n",
      "PROMPT: JakÃ© bylo pÅ¯vodnÃ­ jmÃ©no Karla IV.? OdpovÄ›Ä:\n",
      "MODEL: jakÃ© bylo pÅ¯vodnÃ­ jmÃ©no karla iv.? odpovÄ›Än aplk i. e.s a alxnrih . o ocrs eie eie ei ee ee\n",
      "PROMPT: Otec vlasti, Karel IV., se pÅ¯vodnÄ› jmenoval\n",
      "MODEL: otec vlasti, karel iv., se pÅ¯vodnÄ› jmenovalp rz rdr\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
      "PROMPT: KdyÅ¾ se narodil syn EliÅ¡ky PÅ™emyslovny, dostal jmÃ©no\n",
      "MODEL: kdyÅ¾ se narodil syn eliÅ¡ky pÅ™emyslovny, dostal jmÃ©nodrh a oeie eiee eie aplk   eie alxnreh.\n",
      "vrtlvr eie\n",
      "\n",
      "\n",
      "========================================\n",
      "TOPIC: NejvyÅ¡Å¡Ã­ hora ÄŒR\n",
      "========================================\n",
      "PROMPT: Jak se jmenuje nejvyÅ¡Å¡Ã­ hora v ÄŒeskÃ© republice? OdpovÄ›Ä:\n",
      "MODEL: jak se jmenuje nejvyÅ¡Å¡Ã­ hora v ÄeskÃ© republice? odpovÄ›Än\n",
      "apd.\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
      "PROMPT: NejvyÅ¡Å¡Ã­ vrchol ÄŒeska je\n",
      "MODEL: nejvyÅ¡Å¡Ã­ vrchol Äeska jevÅ¡kvc    er e aeo   e e e    eree  eie e  ee   o o\n",
      "PROMPT: Vrchol, kterÃ½ v KrkonoÅ¡Ã­ch dosahuje vÃ½Å¡ky 1603 m n. m., se nazÃ½vÃ¡\n",
      "MODEL: vrchol, kterÃ½ v krkonoÅ¡Ã­ch dosahuje vÃ½Å¡ky 1603 m n. m., se nazÃ½vÃ¡m ataa\n",
      "aa\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for topic, questions in in_domain_questions.items():\n",
    "    print(\"=\"*40)\n",
    "    print(f\"TOPIC: {topic}\")\n",
    "    print(\"=\"*40)\n",
    "    for q in questions:\n",
    "        print_wrapped(f\"PROMPT: {q}\")\n",
    "        generated_text = generate_from_prompt(q, max_new_chars=50, temperature=0.2)\n",
    "        print_wrapped(f\"MODEL: {generated_text}\\n\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Few-Shot Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <<< STEP 3: UPDATE THE BENCHMARK RUNNER >>>\n",
    "def run_benchmark(benchmark_name, few_shot_prefix, benchmark_data, parse_function):\n",
    "    \"\"\"\n",
    "    Runs a few-shot benchmark and prints the accuracy, using correct preprocessing.\n",
    "    \"\"\"\n",
    "    correct_count = 0\n",
    "    total_count = len(benchmark_data)\n",
    "    results = []\n",
    "\n",
    "    print(\"=\"*50)\n",
    "    print(f\"RUNNING BENCHMARK: {benchmark_name}\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    for question, true_answer in tqdm(benchmark_data.items()):\n",
    "        prompt = few_shot_prefix + parse_function(question=question)\n",
    "        full_output = generate_from_prompt(prompt, max_new_chars=25, temperature=0.1, top_k=1)\n",
    "        \n",
    "        # The prompt passed to the parse function must also be the cleaned one\n",
    "        cleaned_prompt = clean_text(prompt)\n",
    "        model_answer = parse_function(raw_output=full_output, prompt=cleaned_prompt)\n",
    "        \n",
    "        # --- THIS IS THE CRUCIAL NEW LINE ---\n",
    "        # Clean the ground-truth answer to ensure a fair comparison\n",
    "        cleaned_true_answer = clean_text(true_answer)\n",
    "        # --- END NEW LINE ---\n",
    "\n",
    "        # Compare answers. The model's output is already lowercase.\n",
    "        is_correct = (model_answer == cleaned_true_answer)\n",
    "        if is_correct:\n",
    "            correct_count += 1\n",
    "        \n",
    "        results.append({\n",
    "            \"question\": question, # Log the original question for readability\n",
    "            \"true_answer\": cleaned_true_answer,\n",
    "            \"model_answer\": model_answer,\n",
    "            \"is_correct\": is_correct\n",
    "        })\n",
    "\n",
    "    # The rest of the function (displaying results) is the same...\n",
    "    accuracy = (correct_count / total_count) * 100\n",
    "    print(f\"\\n--- BENCHMARK '{benchmark_name}' COMPLETE ---\")\n",
    "    print(f\"Accuracy: {accuracy:.2f}% ({correct_count} / {total_count})\")\n",
    "\n",
    "    # Display some examples\n",
    "    print(\"\\n--- Examples of Correct Answers ---\")\n",
    "    correct_examples = [r for r in results if r['is_correct']][:3]\n",
    "    if not correct_examples: print(\"None in this run.\")\n",
    "    for ex in correct_examples: print(f\"Q: {ex['question']} -> A: {ex['model_answer']} (Correct)\")\n",
    "\n",
    "    print(\"\\n--- Examples of Incorrect Answers ---\")\n",
    "    incorrect_examples = [r for r in results if not r['is_correct']][:3]\n",
    "    if not incorrect_examples: print(\"None in this run.\")\n",
    "    for ex in incorrect_examples: print(f\"Q: {ex['question']} -> A: {ex['model_answer']} (Expected: {ex['true_answer']})\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark 1: Factual Knowledge (Country -> Capital)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_capital_prefix = \"NÄ›mecko -> BerlÃ­n\\nPolsko -> VarÅ¡ava\\nUSA -> Washington, D.C.\\n\"\n",
    "country_capital_data = {\n",
    "    \"Slovensko\": \"Bratislava\", \"Rakousko\": \"VÃ­deÅˆ\", \"Francie\": \"PaÅ™Ã­Å¾\",\n",
    "    \"ItÃ¡lie\": \"Å˜Ã­m\", \"SpojenÃ© krÃ¡lovstvÃ­\": \"LondÃ½n\", \"Japonsko\": \"Tokio\",\n",
    "    \"ÄŒÃ­na\": \"Peking\", \"MaÄarsko\": \"BudapeÅ¡Å¥\", \"Å panÄ›lsko\": \"Madrid\",\n",
    "    \"Kanada\": \"Ottawa\", \"AustrÃ¡lie\": \"Canberra\", \"BrazÃ­lie\": \"BrasÃ­lia\",\n",
    "    \"Indie\": \"NovÃ© DillÃ­\", \"Egypt\": \"KÃ¡hira\", \"Finsko\": \"Helsinky\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_country_capital(question=None, raw_output=None, prompt=None):\n",
    "    if question:\n",
    "        return f\"{question} - \"\n",
    "    if raw_output:\n",
    "        generated_part = raw_output[len(prompt):]\n",
    "        return generated_part.split('\\n')[0].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "RUNNING BENCHMARK: Country -> Capital (Corrected)\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fe9323e6a4f454eaefd8f778e491818",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- BENCHMARK 'Country -> Capital (Corrected)' COMPLETE ---\n",
      "Accuracy: 0.00% (0 / 15)\n",
      "\n",
      "--- Examples of Correct Answers ---\n",
      "None in this run.\n",
      "\n",
      "--- Examples of Incorrect Answers ---\n",
      "Q: Slovensko -> A: vrhÄc     er    e e    er (Expected: bratislava)\n",
      "\n",
      "Q: Rakousko -> A: vrhasa      erkln    ekn (Expected: vÃ­deÅˆ)\n",
      "\n",
      "Q: Francie -> A: vrtlvr     ert   etal   e (Expected: paÅ™Ã­Å¾)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "run_benchmark(\n",
    "    benchmark_name=\"Country -> Capital (Corrected)\",\n",
    "    few_shot_prefix=country_capital_prefix,\n",
    "    benchmark_data=country_capital_data,\n",
    "    parse_function=parse_country_capital\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark 2: Grammatical Skill (Singular -> Plural)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "singular_plural_prefix = \"pes (sg) -> psi (pl)\\nÅ¾ena (sg) -> Å¾eny (pl)\\nmÄ›sto (sg) -> mÄ›sta (pl)\\n\"\n",
    "singular_plural_data = {\n",
    "    \"hrad\": \"hrady\", \"muÅ¾\": \"muÅ¾i\", \"stroj\": \"stroje\", \"soudce\": \"soudci\",\n",
    "    \"Å¡kola\": \"Å¡koly\", \"ulice\": \"ulice\", \"pÃ­seÅˆ\": \"pÃ­snÄ›\", \"kost\": \"kosti\",\n",
    "    \"moÅ™e\": \"moÅ™e\", \"kuÅ™e\": \"kuÅ™ata\", \"nÃ¡draÅ¾Ã­\": \"nÃ¡draÅ¾Ã­\", \"svÄ›tlo\": \"svÄ›tla\",\n",
    "    \"letadlo\": \"letadla\", \"auto\": \"auta\", \"nÃ¡mÄ›stÃ­\": \"nÃ¡mÄ›stÃ­\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_singular_plural(question=None, raw_output=None, prompt=None):\n",
    "    if question:\n",
    "        return f\"{question} sg - \"\n",
    "    if raw_output:\n",
    "        generated_part = raw_output[len(prompt):]\n",
    "        answer = generated_part.replace(\"pl\", \"\").strip()\n",
    "        return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "RUNNING BENCHMARK: Singular -> Plural (Corrected)\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7214b67a40d2403480ed0b2af9d329b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- BENCHMARK 'Singular -> Plural (Corrected)' COMPLETE ---\n",
      "Accuracy: 0.00% (0 / 15)\n",
      "\n",
      "--- Examples of Correct Answers ---\n",
      "None in this run.\n",
      "\n",
      "--- Examples of Incorrect Answers ---\n",
      "Q: hrad -> A: Å¾n e e  e   e   e  e   er (Expected: hrady)\n",
      "\n",
      "Q: muÅ¾ -> A: Å¾n e e    er e e   e   e (Expected: muÅ¾i)\n",
      "\n",
      "Q: stroj -> A: Å¾n e e  e   e   e   er e (Expected: stroje)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "run_benchmark(\n",
    "    benchmark_name=\"Singular -> Plural (Corrected)\",\n",
    "    few_shot_prefix=singular_plural_prefix,\n",
    "    benchmark_data=singular_plural_data,\n",
    "    parse_function=parse_singular_plural\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alignment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
